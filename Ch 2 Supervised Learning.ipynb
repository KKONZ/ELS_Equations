{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Models and Least Squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given inputs $x^T = (X_1,X_2,...,X_p)$, we predict the output Y via the model:\n",
    "\n",
    "$\\hat{Y} = \\hat{\\beta}_0 + \\sum\\limits_{j=1}^{p}X_j\\hat{\\beta}_j$\n",
    "\n",
    "$\\hat{\\beta}_0$ is the intercept, also known as the _bias_ in machine learning. \n",
    "\n",
    "Often it is convenient to include the constant variable 1 in $X$, include $\\hat{\\beta}_0$ in the vector of coefficients $\\hat{\\beta}$, and then write the linear model in vector form as an inner product.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\hat{Y} = X^{T}\\hat{\\beta}$,\n",
    "\n",
    "$X^T$ denotes vector or matrix transpose ($X$ being a column vector). Here we are modeling a single output, so $\\hat{Y}$ is scalar; in general $\\hat{Y}$ can be a K-vector, in which vase $\\beta$ would be a $p\\times{} K$ matrix of coefficients. In the $(p+1)$-dimensional input-output space, $(X,\\hat{Y})$ represents a hyperplane. If the constant is included in $X$, then the hyperplane includes the origin and is a subspace; if not, it is an affine set cutting the Y-axis at the point $(0,\\hat{\\beta}_0)$. From now on we assume that the intercept is included in $\\hat{\\beta}$.\n",
    "\n",
    "Viewed as a function over the p-dimensional input space, $f(X) = X^T\\beta{}$ is linear, and the gradient $f^\\prime(X) = \\beta$ is a vector in input space that points in the steepest uphill direction.\n",
    "\n",
    "There are many different methods to fit a linear model to training data. The _least squares_ is the most popular. In this approach, we pick the coefficients $\\beta$ to minimize the residual sum of squares\n",
    "\n",
    "$RSS(\\beta) = \\sum\\limits_{i=1}^{N}(y_i - x_i^T\\beta)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$RSS(\\beta)$ is a quadratic function of the parameters, and hence its minimum always exists, but may not be unique. The solution is easiest to characterize in the matrix notation. We can write\n",
    "\n",
    "$RSS(\\beta) = (y - X\\beta)^T(y-X\\beta)$,\n",
    "\n",
    "where $X$ is an N x p matrix with each row an input vector, and $y$ is an N-vector of the outputs in the training set. Defferentitating w.r.t. $\\beta$ we get the _normal equations_\n",
    "\n",
    "$X^T(y-X\\beta) = 0$\n",
    "\n",
    "If $X^TX$ is nonsingular, then the unique solution is given by\n",
    "\n",
    "$\\hat{\\beta} = (X^TX)^-1X^Ty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
