{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Models and Least Squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given inputs $x^T = (X_1,X_2,...,X_p)$, we predict the output Y via the model:\n",
    "\n",
    "$\\hat{Y} = \\hat{\\beta}_0 + \\sum\\limits_{j=1}^{p}X_j\\hat{\\beta}_j$\n",
    "\n",
    "$\\hat{\\beta}_0$ is the intercept, also known as the _bias_ in machine learning. \n",
    "\n",
    "Often it is convenient to include the constant variable 1 in $X$, include $\\hat{\\beta}_0$ in the vector of coefficients $\\hat{\\beta}$, and then write the linear model in vector form as an inner product.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\hat{Y} = X^{T}\\hat{\\beta}$,\n",
    "\n",
    "$X^T$ denotes vector or matrix transpose ($X$ being a column vector). Here we are modeling a single output, so $\\hat{Y}$ is scalar; in general $\\hat{Y}$ can be a K-vector, in which vase $\\beta$ would be a $p\\times{} K$ matrix of coefficients. In the $(p+1)$-dimensional input-output space, $(X,\\hat{Y})$ represents a hyperplane. If the constant is included in $X$, then the hyperplane includes the origin and is a subspace; if not, it is an affine set cutting the Y-axis at the point $(0,\\hat{\\beta}_0)$. From now on we assume that the intercept is included in $\\hat{\\beta}$.\n",
    "\n",
    "Viewed as a function over the p-dimensional input space, $f(X) = X^T\\beta{}$ is linear, and the gradient $f^\\prime(X) = \\beta$ is a vector in input space that points in the steepest uphill direction.\n",
    "\n",
    "There are many different methods to fit a linear model to training data. The _least squares_ is the most popular. In this approach, we pick the coefficients $\\beta$ to minimize the residual sum of squares\n",
    "\n",
    "$RSS(\\beta) = \\sum\\limits_{i=1}^{N}(y_i - x_i^T\\beta)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$RSS(\\beta)$ is a quadratic function of the parameters, and hence its minimum always exists, but may not be unique. The solution is easiest to characterize in the matrix notation. We can write\n",
    "\n",
    "$RSS(\\beta) = (y - X\\beta)^T(y-X\\beta)$,\n",
    "\n",
    "where $X$ is an N x p matrix with each row an input vector, and $y$ is an N-vector of the outputs in the training set. Defferentitating w.r.t. $\\beta$ we get the _normal equations_\n",
    "\n",
    "$X^T(y-X\\beta) = 0$\n",
    "\n",
    "If $X^TX$ is nonsingular, then the unique solution is given by\n",
    "\n",
    "$\\hat{\\beta} = (X^TX)^-1X^Ty$,\n",
    "\n",
    "and the fitted value at the ith input _$x_i$_ is $\\hat{y}_i = \\hat{y}(x_i) = x_i^T\\hat{\\beta}$. At an arbitrary input $x_o$ the prediction is $\\hat{y}(x_0) = x_0^T\\hat{\\beta}$. The entire fitted surface is characterized by the p parameters $\\hat{\\beta}$. Intuitively, it seems that we do not need a very large data set to fit such a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest-Neighbor Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest-neighbor methods use those observations in the training set $\\tau$ closest in input space to $x$ to form $\\hat{Y}$. Specifically, the k-nearest neighbor fit for $\\hat{Y}$ is as follows:\n",
    "\n",
    "$\\hat{Y}(x) = \\frac{1}{k}\\sum\\limits_{x_i\\in{}N_k(x)}y_i$\n",
    "\n",
    "Where $N_k(X)$ is the neighborhood of $x$ defined by the $k$ closest points $x_i$ in the training sample. Closeness implies a metric, which for the moment we assume is Euclidean distance. So, in words, we find the $k$ observations with $x_i$ closest to $x$ in input space, and average their responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Statistical Decision Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scetion we develop a small amount of theory that provides a framework for developing models such as those discussed informally so far. We first consider the case of a quantitative output, and place ourselves in the world of random variables and probabililty spaces. Let $X\\in\\rm I\\!R^p$ a real valued random output variable, with joint distribution $Pr(X,Y)$. We seek a function $f(X)$ for predicting $Y$ given values of the input $X$. This theory requires a _loss function_ and convenient is _squared error loss_: $L(Y,f(X)) = (Y - f(X))^2$. This leads us to a criterion for choosing $f$,\n",
    "\n",
    "$EPE = E(Y-f(X))^2$\n",
    "\n",
    "= $\\int{[y - f(x)]^2}Pr(dx,dy)$,\n",
    "\n",
    "the expected (squared) prediction error. By conditioning, factoring the joint density $Pr(X,Y) = Pr(Y|X)Pr(X)$ where $Pr(Y|X) = Pr(Y,X)/Pr(X)$, and splictting the bivariate integral accordingly, on $X$ we can write $EPE$ as\n",
    "\n",
    "$EPE(f) = E_XE_{Y|X}([Y-f(X)]^2|X)$\n",
    "\n",
    "and we see that it suffices to minimize EPE pointwise:\n",
    "\n",
    "$f(x) = argmin_cE_{Y|X}([Y-c]^2|X=x)$\n",
    "\n",
    "The solution is \n",
    "\n",
    "$f(x) = E(Y|X = x)$\n",
    "\n",
    "the conditional expectation, also known as the _regression_ function.\n",
    "\n",
    "Tus the best prediction of $Y$ at any point $X = x$ is a conditional mean, when best is measured by average squared error.\n",
    "\n",
    "The nearest-neighbor methods attempt to directly implement this recipe using the training data. At each point $x$, we might ask for the average of all those $y_i$s with input $x_i = x$. Since there is typically at most one observation at any point $x$, we settle for\n",
    "\n",
    "$\\hat(f) = Ave(y_i|x_i\\in{}N_k(x))$\n",
    "\n",
    "where \"Ave\" denotes the average, and $N_k(x)$ is the neighborhood containing the $k$ points in $\\tau$ closest to $x$. Two approximations are happening here:\n",
    "\n",
    "* expectation is approximated by averaging over sample data;\n",
    "* conditiong at a point is relaxed to conditioning on some region \"close\" to target point.\n",
    "\n",
    "For large sample size $N$, the points in the neighborhood are likely to be close to $x$, and as $k$ gets large the average will get more stable. In fact, under mild regularity conditions on the joint probability distribution $Pr(X,Y)$, one can show that as $N, k \\rightarrow\\infty{}$ such that $k/N \\rightarrow{0}$, $\\hat{f} \\rightarrow{}E(Y|X = x)$. In light of this, why look further, since it seems we have a universal approximator? We often do not have very large samples. If the linear or some more structured model is appropriate, then we can usually get a more stable estimate than _k_-nearest neighbors, although such knowledge has to be learned from the data as well. There are other problems though, sometimes disastrous. As the dimension _p_ gets large, so does the metric size of the _k_-nearest nearest neighborhood. So settling for nearest neighborhood as a surrogate for conditioning will fail us miserably. The convergence above still holds, but the _rate_ of convergence decreases as the dimension increases.\n",
    "\n",
    "How does linear regression fit into this framework? The simplest explanation is that one assumes that the regression function $f(x)$ is approximately linear in its arguments:\n",
    "\n",
    "\n",
    "$f(x)\\approx{x^T\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a model-based approach- we specify a model for the regression function. Plugging this linear model for $f(x)$ into EPE and differentiating we can solve for $\\beta$ theoretically:\n",
    "    \n",
    "$\\beta = [E(XX^T)]^{-1}E(XY)$\n",
    "\n",
    "Note that we have _not_ conditioned on $X$; rather we have used our knowledge of the functional relationship to _pool_ over values of $X$. The least squares solution amounts to replacing the expectation in by averages over the training data.\n",
    "\n",
    "So both _k_-nearest neighbors and least squares end up approximating conditional expectations by averages. But they differ dramatically in terms of model assumptions:\n",
    "\n",
    "* Least squares assume $f(x)$ is well approximated by a globally linear function\n",
    "* _k_-nearest neighbors assumes $f(x)$ is well approximated by a locally constant function.\n",
    "\n",
    "Although the latter seems more palatable, we have already seen that we may pay a price for this flexibility.\n",
    "Many of the modern techniques described in this book are model based, although far more flexible than the rigid linear model. For example, additive models assume that \n",
    "\n",
    "$f(X) = \\sum\\limits_{j=1}^{p}f_j(X_j)$.\n",
    "\n",
    "This retains the additivity of the linear model, but each coordinate function $f_j$ is arbitrary. It turns out that the optimal estimate for the additive model uses techniques such as _k_-nearest neighbors to approximate _univariate_ conditional expectations _simultaneously_ for each of the coordinate functions. Thus the problems of estimating a conditional expectation in high dimensions are swept aay in this case by imposing some (often unrealistic) model assumptions, in this case additivity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
