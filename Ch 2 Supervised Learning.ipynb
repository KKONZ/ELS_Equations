{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Models and Least Squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given inputs $x^T = (X_1,X_2,...,X_p)$, we predict the output Y via the model:\n",
    "\n",
    "$\\hat{Y} = \\hat{\\beta}_0 + \\displaystyle\\sum_{j=1}^{p}X_j\\hat{\\beta}_j$\n",
    "\n",
    "\n",
    "$\\hat{\\beta}_0$ is the intercept, also known as the _bias_ in machine learning. \n",
    "\n",
    "Often it is convenient to include the constant variable 1 in $X$, include $\\hat{\\beta}_0$ in the vector of coefficients $\\hat{\\beta}$, and then write the linear model in vector form as an inner product.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\hat{Y} = X^{T}\\hat{\\beta}$,\n",
    "\n",
    "$X^T$ denotes vector or matrix transpose ($X$ being a column vector). Here we are modeling a single output, so $\\hat{Y}$ is scalar; in general $\\hat{Y}$ can be a K-vector, in which vase $\\beta$ would be a $p\\times{} K$ matrix of coefficients. In the $(p+1)$-dimensional input-output space, $(X,\\hat{Y})$ represents a hyperplane. If the constant is included in $X$, then the hyperplane includes the origin and is a subspace; if not, it is an affine set cutting the Y-axis at the point $(0,\\hat{\\beta}_0)$. From now on we assume that the intercept is included in $\\hat{\\beta}$.\n",
    "\n",
    "Viewed as a function over the p-dimensional input space, $f(X) = X^T\\beta{}$ is linear, and the gradient $f^\\prime(X) = \\beta$ is a vector in input space that points in the steepest uphill direction.\n",
    "\n",
    "There are many different methods to fit a linear model to training data. The _least squares_ is the most popular. In this approach, we pick the coefficients $\\beta$ to minimize the residual sum of squares\n",
    "\n",
    "$RSS(\\beta) = \\displaystyle\\sum_{i=1}^{N}(y_i - x_i^T\\beta)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$RSS(\\beta)$ is a quadratic function of the parameters, and hence its minimum always exists, but may not be unique. The solution is easiest to characterize in the matrix notation. We can write\n",
    "\n",
    "$RSS(\\beta) = (y - X\\beta)^T(y-X\\beta)$,\n",
    "\n",
    "where $X$ is an N x p matrix with each row an input vector, and $y$ is an N-vector of the outputs in the training set. Defferentitating w.r.t. $\\beta$ we get the _normal equations_\n",
    "\n",
    "$X^T(y-X\\beta) = 0$\n",
    "\n",
    "If $X^TX$ is nonsingular, then the unique solution is given by\n",
    "\n",
    "$\\hat{\\beta} = (X^TX)^-1X^Ty$,\n",
    "\n",
    "and the fitted value at the ith input _$x_i$_ is $\\hat{y}_i = \\hat{y}(x_i) = x_i^T\\hat{\\beta}$. At an arbitrary input $x_o$ the prediction is $\\hat{y}(x_0) = x_0^T\\hat{\\beta}$. The entire fitted surface is characterized by the p parameters $\\hat{\\beta}$. Intuitively, it seems that we do not need a very large data set to fit such a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest-Neighbor Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest-neighbor methods use those observations in the training set $\\tau$ closest in input space to $x$ to form $\\hat{Y}$. Specifically, the k-nearest neighbor fit for $\\hat{Y}$ is as follows:\n",
    "\n",
    "$\\hat{Y}(x) = \\frac{1}{k}\\displaystyle\\sum_{x_i\\in{}N_k(x)}y_i$\n",
    "\n",
    "Where $N_k(X)$ is the neighborhood of $x$ defined by the $k$ closest points $x_i$ in the training sample. Closeness implies a metric, which for the moment we assume is Euclidean distance. So, in words, we find the $k$ observations with $x_i$ closest to $x$ in input space, and average their responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Statistical Decision Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scetion we develop a small amount of theory that provides a framework for developing models such as those discussed informally so far. We first consider the case of a quantitative output, and place ourselves in the world of random variables and probabililty spaces. Let $X\\in\\rm I\\!R^p$ a real valued random output variable, with joint distribution $Pr(X,Y)$. We seek a function $f(X)$ for predicting $Y$ given values of the input $X$. This theory requires a _loss function_ and convenient is _squared error loss_: $L(Y,f(X)) = (Y - f(X))^2$. This leads us to a criterion for choosing $f$,\n",
    "\n",
    "$EPE = E(Y-f(X))^2$\n",
    "\n",
    "= $\\int{[y - f(x)]^2}Pr(dx,dy)$,\n",
    "\n",
    "the expected (squared) prediction error. By conditioning, factoring the joint density $Pr(X,Y) = Pr(Y|X)Pr(X)$ where $Pr(Y|X) = Pr(Y,X)/Pr(X)$, and splictting the bivariate integral accordingly, on $X$ we can write $EPE$ as\n",
    "\n",
    "$EPE(f) = E_XE_{Y|X}([Y-f(X)]^2|X)$\n",
    "\n",
    "and we see that it suffices to minimize EPE pointwise:\n",
    "\n",
    "$f(x) = argmin_cE_{Y|X}([Y-c]^2|X=x)$\n",
    "\n",
    "The solution is \n",
    "\n",
    "$f(x) = E(Y|X = x)$\n",
    "\n",
    "the conditional expectation, also known as the _regression_ function.\n",
    "\n",
    "Tus the best prediction of $Y$ at any point $X = x$ is a conditional mean, when best is measured by average squared error.\n",
    "\n",
    "The nearest-neighbor methods attempt to directly implement this recipe using the training data. At each point $x$, we might ask for the average of all those $y_i$s with input $x_i = x$. Since there is typically at most one observation at any point $x$, we settle for\n",
    "\n",
    "$\\hat(f) = Ave(y_i|x_i\\in{}N_k(x))$\n",
    "\n",
    "where \"Ave\" denotes the average, and $N_k(x)$ is the neighborhood containing the $k$ points in $\\tau$ closest to $x$. Two approximations are happening here:\n",
    "\n",
    "* expectation is approximated by averaging over sample data;\n",
    "* conditiong at a point is relaxed to conditioning on some region \"close\" to target point.\n",
    "\n",
    "For large sample size $N$, the points in the neighborhood are likely to be close to $x$, and as $k$ gets large the average will get more stable. In fact, under mild regularity conditions on the joint probability distribution $Pr(X,Y)$, one can show that as $N, k \\rightarrow\\infty{}$ such that $k/N \\rightarrow{0}$, $\\hat{f} \\rightarrow{}E(Y|X = x)$. In light of this, why look further, since it seems we have a universal approximator? We often do not have very large samples. If the linear or some more structured model is appropriate, then we can usually get a more stable estimate than _k_-nearest neighbors, although such knowledge has to be learned from the data as well. There are other problems though, sometimes disastrous. As the dimension _p_ gets large, so does the metric size of the _k_-nearest nearest neighborhood. So settling for nearest neighborhood as a surrogate for conditioning will fail us miserably. The convergence above still holds, but the _rate_ of convergence decreases as the dimension increases.\n",
    "\n",
    "How does linear regression fit into this framework? The simplest explanation is that one assumes that the regression function $f(x)$ is approximately linear in its arguments:\n",
    "\n",
    "\n",
    "$f(x)\\approx{x^T\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a model-based approach- we specify a model for the regression function. Plugging this linear model for $f(x)$ into EPE and differentiating we can solve for $\\beta$ theoretically:\n",
    "    \n",
    "$\\beta = [E(XX^T)]^{-1}E(XY)$\n",
    "\n",
    "Note that we have _not_ conditioned on $X$; rather we have used our knowledge of the functional relationship to _pool_ over values of $X$. The least squares solution amounts to replacing the expectation in by averages over the training data.\n",
    "\n",
    "So both _k_-nearest neighbors and least squares end up approximating conditional expectations by averages. But they differ dramatically in terms of model assumptions:\n",
    "\n",
    "* Least squares assume $f(x)$ is well approximated by a globally linear function\n",
    "* _k_-nearest neighbors assumes $f(x)$ is well approximated by a locally constant function.\n",
    "\n",
    "Although the latter seems more palatable, we have already seen that we may pay a price for this flexibility.\n",
    "Many of the modern techniques described in this book are model based, although far more flexible than the rigid linear model. For example, additive models assume that \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$f(X) = \\displaystyle\\sum_{j=1}^{p}f_j(X_j)$.\n",
    "\n",
    "This retains the additivity of the linear model, but each coordinate function $f_j$ is arbitrary. It turns out that the optimal estimate for the additive model uses techniques such as _k_-nearest neighbors to approximate _univariate_ conditional expectations _simultaneously_ for each of the coordinate functions. Thus the problems of estimating a conditional expectation in high dimensions are swept aay in this case by imposing some (often unrealistic) model assumptions, in this case additivity.\n",
    "\n",
    "Are we happy with this criterion? What happens if we replace the $L_2$ loss function with the $L_1$: $E|Y-f(X)|$? The solution in this case is the conditional median,\n",
    "\n",
    "$\\hat{f}(x) = median(Y|X = x)$,\n",
    "\n",
    "which is a different measure of loaction, and its estimates are more robust than those for the conditional mean. $L_1$ criteria have discontinuities in their derivaties, which have hindered their widespread use. Other more resistant loss functions will be mentioned in later chapters, but squared error is analytically convenient and the most popular.\n",
    "\n",
    "What do we do when the output is a categorial variable $G$? The same paradigm works here, expect we need a different loss function for penalizing predicition errors. An estimate $\\hat{G}$ will assume values $\\mathscr{G}$, the set of possible classes. Out loss function can be represented by a $K\\times{}K$ matrix $L$, where $K = card(\\mathscr{G})$. $L$ will be zero on the diagonal and nonnegative elsewhere, where $L(\\mathscr{k}, \\mathscr{l})$ is the price paid for classifying an observation belonging to class $\\mathcal{G}_{\\mathscr{k}}$ as $\\mathscr{G}_{\\mathcal{l}}$. Most often we use the _zero-one_ loss function, where all misclassifications are charged a single unit. The expected prediction error is\n",
    "\n",
    "$EPE = E[L(G, \\hat{G}(X))]$,\n",
    "\n",
    "where again the expectation is taken with respect to the joint distribution $Pr*G,X)$. Again we condition, and can write EPE as\n",
    "\n",
    "$EPE = E_X\\displaystyle\\sum_{k=1}^{K}L[\\mathscr{G}_K, \\hat{G}(X)]Pr(\\mathscr{G}_K|X)$\n",
    "\n",
    "and again it suffices to minimize EPE pointwise:\n",
    "\n",
    "$\\hat{G}(x) = argmin_{g\\in\\mathscr{g}}\\displaystyle\\sum_{k=1}^{K}L(\\mathscr{G}_k, g)Pr(\\mathcal{G}_k|X = x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "With the 0-1 loss function this simplifies to \n",
    "\n",
    "$\\hat{G} = \\mathscr{G}_k$ if $Pr(\\mathscr{G}_k|X=x) = \\max\\limits_{g\\in\\mathscr{G}}$ $Pr(g|X=x)$.\n",
    "\n",
    "This reasonable solution is known as the _Bayes classifier_, and says that we classify to the most probable class, using the conditional (discrete) distribution $Pr(G|X)$. The error rate of the Bayes classifier is called the _Bayes rate_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Again we see that the _k_-nearest neighbor classifier directly approximates this solution- a majority voate in a nearest neighborhood amounts to exactly this, except that conditional probability at a point is relaxed to conditional probability within a neighborhood of a point, and probabilities are estimated by training-sample proportions.\n",
    "\n",
    "Suppose for a two-class problem we had taken the dummy-variable approach and coded G via a binary Y, followed by squared error loss estimation. The $\\hat{f(X)} =E(Y|X) = Pr(G = \\mathscr{G}_1|X)$ if $\\mathcal{G}_1$ corresponded to Y = 1. Likewise for a K-class problem, $(Y_k|X) = Pr(G = \\mathscr{G}_k|X)$. This show that our dummy-variable regression procedure, followed by classification to the largest fitted value is another ay of representing the Bayes classifier. Although this theory is exact, in practive problems can occur, depending on the regression model used. For example, when linear regression is used, $\\hat{f}(X)$ need not be positive, and we might be suspicious about using it as an estimate of a probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It would seem that with a reasonably large set of training data, we could always approximate the theoretically optimal conditional expectation by _k_-nearest neighbor averaging, seince we should be able to find a fairly large neighborhood of observations close to any x and average them. This approach and our intuition breaks down in high dimensions, and the phenomenon is commonly referred to as the _curse of dimensionality_. There are many manifestations of this problem, and we will examine a few here.\n",
    "\n",
    "Consider the nearest-neighbor procedure for inputs uniformly distributed in a _p_-dimensional unit hypercube,. Suppose we send out a hypercubical neighborhood about a target point to capture a fraction _r_ of the observations since this corresponds to a fraction _r_ of the unit volume, the expected edge length will be $e_p(r) = r^(1/p)$. In ten dimensions $e_10(0.01) = 0.63$ and $e_10(0.1) = .8$, while the entire range for each input is only 1.0.So to capture 1% or 10% of the data to form a local average, we must cover 63% or 80% of the range of each input variable. Such neighborhoods are no longer \"local\". Reducing _r_ dramatically does not help much either, since the fewer observations we average, the higher is the variance of our fit.\n",
    "\n",
    "Another consequence of the sparse sampling in high dimensions is that all sample points are close to an edge of the sample. Consider _N_ data points uniformly distributed in a _p_-dimensional unit ball centered at the origin. Suppose we consider a nearest-neighbor estimate at the origin. The median distance from the origin to the closestdata point is given by the expression \n",
    "\n",
    "$d(p,N) = (1 - \\frac{1}{2}^{\\frac{1}{N}})^{\\frac{1}{p}}$\n",
    "\n",
    "A more complicated expression exists for the mean distance to the closest point. For $N = 500, p = 10, d(p,N) \\approx{0.52}$, more than halfway to the boundary. Hence most data points are closer to the boundary of the sample space than to any other data point. The reason that this presents a problem is that prediction is much more difficult near the edges of the training sample. One must extrapolate from neighboring sample points rather than interpolate between them. \n",
    "\n",
    "Another manifestation of the curse is that the sampling density is proportional to $N^{\\frac{1}{p}}$, where $p$ is the dimension of the input space and $N$ is the sample size. Thus, if $N_1 = 100$ represents a dense sample for a single input problem, them $n_{10} = 100^{10}$ is the sample size required for the same sampling density with 10 inputs. Thus in high dimensions all feasible training samples sparsely populate the input space.\n",
    "\n",
    "Let us construct another uniform example. Suppose we have 1000 training examples of $x_i$ generated uniformly on $[-1,1]^p$. Assume that the true relationship between $X$ and $Y$ is \n",
    "\n",
    "$Y = f(X) = e^{-8\\parallel{X}\\parallel{}^2}$,\n",
    "\n",
    "without any measurement error. We use the 1-nearest-neighbor rule to predict $y_0$ at the test-point $x_0 = 0$. Denote the training set by $\\tau$. WE compute the expected prediction error at $x_0$ for our procedure, averaging over all such samples of size 1000. Since the problem is deterministic, this is the mean squared error (MSE) for estimating $f(0)$:\n",
    "\n",
    "$MSE(x_0) = E\\tau[f(x_0) - \\hat{y}_0]^2$\n",
    "\n",
    "$= E\\tau[\\hat{y}_0 - E\\tau(\\hat{y}_0)]^2 + [E\\tau(\\hat{y}_0) - f(x_0)]^2$\n",
    "\n",
    "$= Var\\tau(\\hat{y}_0) + Bias^2(\\hat{y}_0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Suppose that we know the relationship between $Y$ and $X$ is linear, \n",
    "\n",
    "$Y = X^T\\beta + \\epsilon$\n",
    "\n",
    "where $\\epsilon \\tilde{}  N(0, \\sigma^2)$ and we fit the model by least squares to the training data. For an arbitrary test point $x_o$, we have $\\hat{y}_0 = x_0^T\\hat{\\beta}$, which can be written as $\\hat{y}0 = x_o^T + \\beta + \\sum_{i=1}^N\\mathscr{l}(x_0)\\epsilon_i$, where $\\mathscr{l}(x_0)$ is the $ith$ element of $X(X^TX)^{-1}x_0$. Since under this model least squares estimates are unbiased, we find that\n",
    "\n",
    "$EPE(x_0) = E_{y_0|x_0}E\\tau(y_0-\\hat{y}_0)^2$\n",
    "\n",
    "$= Var(y_0|x_0) + E\\tau[\\hat{y}_0 - E\\tau\\hat{y}_0]^2 + [E\\tau\\hat{y}_0 - x_0^T\\beta]^2$\n",
    "\n",
    "$= Var(y_0|x_0) + Var\\tau(\\hat{y}_0) + Bias^2(\\hat{y}_0)$\n",
    "\n",
    "$= \\sigma^2(p/N) + \\sigma^2$\n",
    "\n",
    "Here we see that the expected EPE increases linearly as a function of p, with slope $\\sigma^2/N$. If N is large and/or $\\sigma^2$ is small, this growth in variance is negligible (0 in the deterministic case). By impoising some heavy restrictions on the class of models being fitted, we have aboided the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## A statistical Model for the Joint Distribution Pr(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose in fact that our data arose from a statistical mode\n",
    "\n",
    "$Y = f(x) +\\epsilon$\n",
    "\n",
    "where the random error $\\epsilon$ has $E(\\epsilon) = 0$ and is independent of $X$. Note that for this model, $f(x) = E(X,Y)$, and in fact the conditional distribution $Pr(Y|X)$ depends on $X$ _only_ through the conditional mean $f(x)$.\n",
    "\n",
    "The additive error model is a useful approximation to the truth. For most systems the input-output paris $(X,Y)$ will not have a deterministic relationship $Y = f(X)$. Generally there will be other unmeasured variables that also contribute to Y, including measurement error. The additive model assumes that we can capture all these departures from a determinist relationship via the error $\\epsilon$.\n",
    "\n",
    "For some problems a deterministic relationship does hold. Many of the classification problems studied in machine learning are of this form, where the response surface can be thought of as a colored map definied in $\\rm I\\!R^p$. The training data consist of colored examples from the map ${x_i, g_i}$, and the goal is to be able to color any point. Here the function is deterministic, and the randomness enters through the $x$ location of the training points. For the moment we will not puruse such problems, but will see that they can be handled by techniques appropriate for the error-based models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The assumption that the errors are independent and identically distributed is not strictly necessary, but seems to be at the back of our minds when we average squared errors uniformly in our EPE criterion. With such a model it becomes natural to use least squares as a data criterion for model estimation. Simple modifications can be made to avoid the independence assumption; for example, we can have $Var(Y|X = x) = \\sigma{(x)}$, and now both the mean and variance depend on $X$. In general the conditional distribution $Pr(Y|X)$ can depend on $X$ in complicated ways, but the additive error model precludes these.\n",
    "\n",
    "So far we have concentrated on the quantitative response. Additive error models are typically not used for qualitative outputs $G$; in this case the target function $p(X)$ is the conditional density $Pr(G|X)$, and this is modeled directly. For example, for two-class data, it is often reasonable to assume that the data arise from independent binary trails, with the probability of one particular outcome being $p(X)$, and the other $1 - p(X)$. Thus if $Y$ is the $0-1$ coded version of $G$, then $E(Y|X = x) = p(x)$, but the variance depends on $x$ as well: $Var(Y|X = x) = p(x)[1 - p(x)]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Many of the approximations we will encounter have associated a set of parameters $\\theta$ that can be modified to suit the data at hand. For example, the linear model $f(x) = x^T\\beta$ has $\\theta = \\beta$. Another class of useful approximators can be expressed as _linear basis expansions_\n",
    "\n",
    "$ f_{\\theta}(x) = \\displaystyle\\sum_{k =1}^Kh_k(x)\\theta_k$\n",
    "\n",
    "where the $h_k$ are a suitable set of functions or transformations of the input vector $x$. Traditional examples are polynomial and trigonometric expansions, where for example $h_k$ might be $x_{1}^{2}$, $x1x_2^2$, $cos(x_1)$ and so on. We also encounter nonlinear expansions, such as the sigmoid transformation common to neural network models,\n",
    "\n",
    "$h_k(x) = \\frac{1}{1 + exp(-x^T\\beta{k}})$\n",
    "\n",
    "We can use least squares to estimate the parameters $\\theta$ in $f_\\theta$ as we did for the linear model, by minimizing the residual sum-of-squares\n",
    "\n",
    "$RSS(\\theta) = \\displaystyle\\sum_{i = 1}^N(y_i - f\\theta(x_i))^2$\n",
    "\n",
    "as a function of $\\theta$. This seems a reasonable criterion for an additive error model. In terms of function approximation, we imagine our parameterized function as a surface in p + 1 space, and what we observe are noisy realizations from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "While least squares is generally very convenient, it is not the only criterion used and in some cases would not make much sense. A more general principle for estimation is _maximum likelihood estimation_. Suppose we have a random sampe $y_i, i = 1 ...., N$ from a density $Pr_\\theta(y)$ indexed by some parameters $\\theta$. The log-probability of the observed sample is \n",
    "\n",
    "$L(\\theta) = \\displaystyle\\sum_{i=1}^{N}logPr_{\\theta}(y_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The principle of maximum likelihood assumes that the most reasonable values for $\\theta$ are those for which the probability of the observed sample is largest. Least squares for the additive error model $Y = f\\theta(X) + \\epsilon$, with $\\epsilon \\tilde{} N(0, \\sigma^2)$, is equivalent to maximum likelihood using the conditional likelihood\n",
    "\n",
    "$Pr(Y|X, \\theta) = N(f_\\theta(X), \\sigma^2)$.\n",
    "\n",
    "So although the additional assumption of normality seems more restrictive, the results are the same. The log-likelihood of the data is\n",
    "\n",
    "$L(\\theta) = - \\frac{N}{2}log(2\\pi) - Nlog\\sigma - \\frac{1}{2\\sigma^2}\\displaystyle\\sum_{i=1}^{N}(y_i - f_{\\theta}(x_i))^2$,\n",
    "\n",
    "and the only term involving $\\theta$ is the last, which is $RSS(\\theta)$ up to a scalar negative multiplier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A more interesting example is the multinomial likelihood for the regression function $Pr(G|X)$ for a qualitative output $G$. Suppose we have a model $Pr(G = \\mathcal{G}_k|X = x) = p_{k, \\theta}(x), k = 1,...,K$ for the conditional probability of each class given $X$, indexed by the parameter vector $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then the log-likelihood (also referred to as the cross-entropy) is \n",
    "\n",
    "$L(\\theta) = \\displaystyle\\sum_{i=1}^Nlogp_{gi,\\theta}(x_i)$ \n",
    "\n",
    "and when maximized it delivers values of $\\theta$ that best conform with the data that this likelihood sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Structured Regression Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that although nearest-neighbor and other local methods focus directly on estimating the function at a point, they face problems in high dimensions. They may also be inappropriate even in low dimensions in cases where more structured approaches can make more efficient use of the data. This section introduces classes of such structured approaches. Before we proceed, though, we discuss further the need for such classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulty of the Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the RSS criterion for an arbitrary function f,\n",
    "\n",
    "$RSS(f) = \\displaystyle\\sum_{i=1}^{N}(y_i -f(x_i))^2$.\n",
    "\n",
    "Minimizing leads to infinitely many solutions: any function $\\hat{f}$ passing through the training points $(x_i, y_i)$ is a solution. Any particular solution chosen might be a poor predictor at test points different from the training points. If there are multiple observations pairs $(x_i, y_{i\\mathscr{L}}$, $\\mathscr{L} = 1,...,N_i$ at each value of $x_i$, the risk is limited. In this case, the solutions pass through the average values of the $y_{i, \\mathscr{L}}$ at each$x_i$. If the sample size N were sufficiently large such that repeats were guaranteed and densely arranged it would seem that these solutions might all tend to the limiting conditional expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In order to obtain useful results for finite N, we must restrict the eligible solutions to a smaller set of functions.How to decide on the nature of the restrictions is based on considerations outside of the data. These restrictions are sometimes encoded via the parametric representation of $f_\\theta$, or may be built into the learning method itself, either implicitly or explicitly.These restricted classes of solutions are the major topic of this book.One thing should be clear, though. Any restrictions imposed bon f that lead to a unique solution do not really remove the ambiguity caused by the multiplicity of solutions. There are infinitely many possible restrictions, each leading to a unique solution, so the ambiguity has simply been transferred to the choice of constraint.\n",
    "\n",
    "In general the constraints imposed by most learning methods can be described as _complexity_ restrictions of one kind or another. This usually means some kind of regular behavior in small neighborhoods of the input space. That is, for all input points x sufficiently close to each other in some metric, $\\hat{f}$ exhibits some special structure such as nearly constant, linear or low-order polynomial behavior. The estimator is then obtained by averaging or polynomial fitting in that neighborhood.\n",
    "\n",
    "The strength of the constraint is dictated by the neighborhood size. The larger the size of the neighborhood, the stronger the constraint, and the more sensitive the solution is to the particular choice of constraint. For example, local constant fits in infinitesimally small neighborhoods is no constraint at all; local linear fits in large neighborhoods is almost a globally linear model, and is very restrictive.\n",
    "\n",
    "The nature of the constraint depends on the metric used. Some methods, such as kernel and local regression and tree-based methods, directly specify the metric and size of the neighborhood. The nearest-neighbor methods discussed so far are based on the assumption that locally the function is constant; close to a target input $x_0$, the function does not change much, and so close outputs can be averaged to produce $\\hat{f}(x_o)$. Other methods such as splines, neural networks and basis-function methods implicitlydefine neighborhoods of local behavior. In _equivalent kernel_, which describes this local dependence for any method linear in the outputs. These equivalent kernels in many cases look just like the explicitly defined weighting kernels discussed above- peaked at the target point and falling away smoothly away from it.\n",
    "\n",
    "One fact should be clear by now. Any method that attempts to produce locally varying functions in small isotropic neighborhoods will run into problems in high dimensions-again the curse of dimensionality. And conversely, all methods that overcome the dimensionality problems have an associated-and often implicit or adaptive-metric for measuing neighborhoods, which basically does not allow the neighborhood to be simutaneously small in all directions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classes of Restricted Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variety of nonparametric regression techniques or learning methods fall into a number of different classes depending on the nature of the restrictions imposed. These classes are not distinct, and indeed some methods fall in several classes. Here we give a brief summary, since detailed descriptions are given in later chapters. Each of the classes has associated with it one or more parameters, sometimes appropriately called _smoothing_ parameters that control the effective size of the local neighborhood. Here we describe three broad classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Roughness Penatly and Bayesian Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the class of functions is controlled by explicitly penalizing $RSS(f)$ with a roughness penalty\n",
    "\n",
    "$PRSS(f;\\lambda) = RSS(f) + \\lambda{}J(f)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The user-selected functional $J(f)$ will be large for functions $f$ that vary too rapidly over small regions of input space. For example, the popular _cubic smoothing spline_ for one-dimensional inputs is the solution to the penalized least-squares criterion\n",
    "\n",
    "$PRSS(f; \\lambda) = \\sum\\limits_{i=1}^N(y_i - f(x_i))^2 + \\lambda\\int[f^{\\prime\\prime}(x)]^2dx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The roughness penality here controls large values of the second derivative of f, and the amount of penalty is dictated by $\\lambda \\geq 0$. For $\\lambda = 0$ no penalty is imposed, and any interpolating function will do, while for $\\lambda = \\infty{}$ only functions linear in x are permitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Penalty functionals J can be constructed for functions in any dimension, and special versions can be created to impose special structure. For example, additive penalties $J(f) = \\displaystyle\\sum_{j=1}^pK(f_i)$ are used in conjunction with additive functions $f(X) = \\displaystyle\\sum_{j=1}^pf_j(X_j)$ to create additive models with smooth coordinate functions. Similarly, _projection pursuit regression_ models have $f(X) = \\displaystyle\\sum_{m=1}^Mg_m(\\alpha_m^TX)$ for adaptively chosen directions $\\alpha_m$, and the functions $g_m$ can each have an associated roughness penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Penalty function, or _regularization_ methods, express our prior belief that the type of functions we seek exhibit a certain type of smooth behavior, and indeed can usually be cast in a Bayesian framework. The penaly $J$ corresponds to a log-prior, and $PRSS(f;\\lambda)$ the log-posterior distribution and minimizing $PRSS(f;\\lambda)$ amounts to finding the posterior mode. We discuss roughness-penalty approaches in Chapter 5 and the Bayesian paradigm in Chapter 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
